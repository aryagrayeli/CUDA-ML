Layers: 4
784 -> 128 (sigmoid) -> 64 (sigmoid) -> 10 (softmax)
Train: train_data
Test: test_data
Checkpoint: test.pth
Epochs: 2  
Batch Size: 128
Alpha: 0.01
Loss: CrossEntropy