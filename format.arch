Layers: 4
784 -> 128 (sigmoid) -> 64 (sigmoid) -> 10 (softmax)
Train: train_data
Test: test_data
Checkpoint: weights.pth
Epochs: 15  
Batch Size: 128
Alpha: 0.01
Epsilon: 0.05
Loss: CrossEntropy